{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7d34417",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.scanner import read_daphnet_dataset, flatten_dfs\n",
    "from Utils.preprocess import Pipeline, PreProcessor\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22ac23a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 26 trails for training patient data, and 9 trails for testing patient data.\n"
     ]
    }
   ],
   "source": [
    "# Load the database of patients and trails\n",
    "patients_trails_dfs, feature_columns, label_column, sample_rate = read_daphnet_dataset()\n",
    "\n",
    "\n",
    "# Randomly select 25% of patients for testing\n",
    "num_patients = len(patients_trails_dfs)\n",
    "test_indices = random.sample(range(num_patients), int(num_patients * 0.25))\n",
    "train_indices = [i for i in range(num_patients) if i not in test_indices]\n",
    "train_comb_trails_dfs = flatten_dfs([patients_trails_dfs[i] for i in train_indices])\n",
    "test_comb_trails_dfs = flatten_dfs([patients_trails_dfs[i] for i in test_indices])\n",
    "print(f\"There are {len(train_comb_trails_dfs)} trails for training patient data, \\\n",
    "and {len(test_comb_trails_dfs)} trails for testing patient data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40b1552f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(32, 16, kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(16, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, channels)\n",
    "        x = x.permute(0, 2, 1)  # (batch, channels, seq_len)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool(x)  # (batch, 64, 1)\n",
    "        x = x.squeeze(-1) # (batch, 64)\n",
    "        x = self.fc(x)    # (batch, num_classes)\n",
    "        return x\n",
    "    \n",
    "    def save(self, current_date, directory_name, epoch):\n",
    "        # Save the model\n",
    "        model_name = f\"model_{current_date}_epoch{epoch + 1}.pth\"\n",
    "        save_path = os.path.join(directory_name, model_name)\n",
    "        torch.save(self.state_dict(), save_path)\n",
    "        print(f\"Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5966f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.3720\n",
      "Model saved to Models/Date_20250825_161512_SimpleCNN/model_20250825_161512_epoch1.pth\n",
      "Epoch 2/10, Loss: 0.3370\n",
      "Model saved to Models/Date_20250825_161512_SimpleCNN/model_20250825_161512_epoch2.pth\n",
      "Epoch 3/10, Loss: 0.3268\n",
      "Model saved to Models/Date_20250825_161512_SimpleCNN/model_20250825_161512_epoch3.pth\n",
      "Epoch 4/10, Loss: 0.3196\n",
      "Model saved to Models/Date_20250825_161512_SimpleCNN/model_20250825_161512_epoch4.pth\n",
      "Epoch 5/10, Loss: 0.3162\n",
      "Model saved to Models/Date_20250825_161512_SimpleCNN/model_20250825_161512_epoch5.pth\n",
      "Epoch 6/10, Loss: 0.3126\n",
      "Model saved to Models/Date_20250825_161512_SimpleCNN/model_20250825_161512_epoch6.pth\n",
      "Epoch 7/10, Loss: 0.3097\n",
      "Model saved to Models/Date_20250825_161512_SimpleCNN/model_20250825_161512_epoch7.pth\n",
      "Epoch 8/10, Loss: 0.3067\n",
      "Model saved to Models/Date_20250825_161512_SimpleCNN/model_20250825_161512_epoch8.pth\n",
      "Epoch 9/10, Loss: 0.3021\n",
      "Model saved to Models/Date_20250825_161512_SimpleCNN/model_20250825_161512_epoch9.pth\n",
      "Epoch 10/10, Loss: 0.2985\n",
      "Model saved to Models/Date_20250825_161512_SimpleCNN/model_20250825_161512_epoch10.pth\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters to tune\n",
    "slice_window_size =     4\n",
    "slice_window_stride =   0.5 \n",
    "pre_window_sec =        0\n",
    "\n",
    "# Preprocess the data\n",
    "pp = Pipeline()\n",
    "preprocessor = PreProcessor(pre_window_sec=pre_window_sec, sample_rate=sample_rate)\n",
    "pp.add_step(preprocessor.downsample_dfs_to_50hz)\n",
    "pp.add_step(preprocessor.normalize)\n",
    "p_train_comb_trails_dfs = pp.execute(train_comb_trails_dfs, feature_columns)\n",
    "p_test_comb_trails_dfs = pp.execute(test_comb_trails_dfs, feature_columns)\n",
    "\n",
    "# Slice the data into windows\n",
    "input_windows, raw_target = preprocessor.slice_windows(dfs = p_train_comb_trails_dfs,\n",
    "                                                        window_size = slice_window_size, \n",
    "                                                        stride = slice_window_stride, \n",
    "                                                        feature_columns = feature_columns,\n",
    "                                                        target_cols= label_column)\n",
    "\n",
    "#shuffle the data\n",
    "indices = np.arange(len(input_windows))\n",
    "np.random.shuffle(indices)\n",
    "input_windows = input_windows[indices]\n",
    "raw_target = raw_target[indices]\n",
    "\n",
    "# One-hot encode the target labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "onehot_target = encoder.fit_transform(raw_target)\n",
    "\n",
    "# Prepare data\n",
    "X = torch.tensor(input_windows, dtype=torch.float32)  # (num_samples, seq_len, 9)\n",
    "y = torch.tensor(onehot_target, dtype=torch.float32)  # (num_samples, 3)\n",
    "\n",
    "# Reset the model\n",
    "model = SimpleCNN(input_channels=len(feature_columns), num_classes=onehot_target.shape[1])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Create a directory with the format \"Lnn_Date_YYYYMMDD\"\n",
    "current_date = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "directory_name = f\"Models/Date_{current_date}_SimpleCNN\"\n",
    "if not os.path.exists(directory_name):\n",
    "    os.makedirs(directory_name)\n",
    "# print(directory_name)\n",
    "\n",
    "# Train the SimpleCNN model with input_windows and onehot_target\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "for epoch in range(num_epochs):\n",
    "    permutation = torch.randperm(X.size(0))\n",
    "    epoch_loss = 0.0\n",
    "    for i in range(0, X.size(0), batch_size):\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch_x, batch_y = X[indices], y[indices]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * batch_x.size(0)\n",
    "    avg_loss = epoch_loss / X.size(0)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "    model.save(current_date, directory_name, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a27a478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9393    0.9857    0.9620     10453\n",
      "           1     0.3968    0.1283    0.1939       764\n",
      "\n",
      "    accuracy                         0.9273     11217\n",
      "   macro avg     0.6680    0.5570    0.5779     11217\n",
      "weighted avg     0.9023    0.9273    0.9096     11217\n",
      "\n",
      "Validation CrossEntropyLoss: 0.26292428374290466\n",
      "Macro F1 Score: 0.5779121166953458\n"
     ]
    }
   ],
   "source": [
    "# Slice the validation data into windows\n",
    "val_input_windows, val_raw_target = preprocessor.slice_windows(dfs = p_test_comb_trails_dfs,\n",
    "                                                                window_size = slice_window_size, \n",
    "                                                                stride = slice_window_stride, \n",
    "                                                                feature_columns = feature_columns,\n",
    "                                                                target_cols= label_column)\n",
    "\n",
    "# One-hot encode the validation target labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "onehot_val_target = encoder.fit_transform(val_raw_target)\n",
    "\n",
    "model_dir = directory_name\n",
    "model_files = [f for f in os.listdir(model_dir) if f.endswith(\".pth\")]\n",
    "\n",
    "# Prepare validation data\n",
    "X_val = torch.tensor(val_input_windows, dtype=torch.float32)\n",
    "y_val = np.argmax(onehot_val_target, axis=1)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)  # For CrossEntropyLoss\n",
    "\n",
    "# evaluate the model\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(X_val)\n",
    "    preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "    val_loss = criterion(logits, y_val_tensor).item()\n",
    "    macro_f1 = f1_score(y_val, preds, average=\"macro\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val, preds, digits=4))\n",
    "print(\"Validation CrossEntropyLoss:\", val_loss)\n",
    "print(\"Macro F1 Score:\", f1_score(y_val, preds, average=\"macro\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
